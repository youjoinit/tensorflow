{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络是一种应用广泛的机器学习模型，是存在于计算机内部的神经系统，由大量的神经元相连接并进行计算，这些神经元负责传递信息和加工信息，通过外界信息的输入，不断改变神经网络内部的结构，实现输入和输出间复杂的关系的建模。\n",
    "\n",
    "下图就是一个简单的神经网络系统，输入层负责接收信息，比如说一只猫的图片；输出层就是计算机对这个输入信息的认知，判断它是不是猫。隐藏层就是对输入信息的加工处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://dn-anything-about-doc.qbox.me/document-uid440821labid3270timestamp1500367644192.png/wm' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于神经网络的训练，首先需要很多数据。比如他要判断一张图片是不是猫。就要输入上千万张的带有标签的猫猫狗狗的图片，然后再训练上千万次。\n",
    "\n",
    "神经网络训练的结果有对的也有错的，如果是错误的结果，将被当做非常宝贵的经验，那么是如何从经验中学习的呢？就是对比正确答案和错误答案之间的区别，然后把这个区别反向的传递回去，对每个相应的神经元进行一点点的改变。那么下一次在训练的时候就可以用已经改进一点点的神经元去得到稍微准确一点的结果。\n",
    "\n",
    "每个神经元都有属于它的激活函数，用这些函数给计算机一个刺激行为。\n",
    "\n",
    "比如，在第一次给计算机看猫的图片的时候，只有部分的神经元被激活，被激活的神经元所传递的信息是对输出结果最有价值的信息。如果输出的结果被判定为是狗，也就是说是错误的了，那么就会修改神经元，对于前一次一些容易被激活的神经元会变得迟钝，另外一些神经元会变得敏感。这样一次次的训练下去，所有神经元的参数都在被改变，它们变得对真正重要的信息更为敏感。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://dn-anything-about-doc.qbox.me/document-uid440821labid3270timestamp1500370890978.png/wm' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激励函数\n",
    "\n",
    "例如一个神经元对猫的眼睛敏感，那当它看到猫的眼睛的时候，就被激励了，相应的参数就会被调优，它的贡献就会越大。\n",
    "\n",
    "下面是几种常见的激活函数：\n",
    "\n",
    "x轴表示传递过来的值，y轴表示它传递出去的值：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://dn-anything-about-doc.qbox.me/document-uid440821labid3270timestamp1500429806697.png/wm'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "基本流程\n",
    "\n",
    "定义添加神经层的函数\n",
    "\n",
    "(1) 训练的数据\n",
    "\n",
    "(2) 定义节点准备接收数据\n",
    "\n",
    "(3) 定义神经层：隐藏层和预测层\n",
    "\n",
    "(4) 定义 loss 表达式\n",
    "\n",
    "(5) 选择 optimizer 使 loss 达到最小\n",
    "\n",
    "然后对所有变量进行初始化，通过 sess.run optimizer，迭代 1000 次进行学习。\n",
    "\n",
    "来源: 实验楼\n",
    "链接: https://www.shiyanlou.com/courses/893\n",
    "本课程内容，由作者授权实验楼发布，未经允许，禁止转载、下载及非法传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.63676\n",
      "0.391387\n",
      "0.282629\n",
      "0.202204\n",
      "0.170054\n",
      "0.143901\n",
      "0.150855\n",
      "0.145063\n",
      "0.139137\n",
      "0.131579\n",
      "0.138607\n",
      "0.139657\n",
      "0.116037\n",
      "0.12813\n",
      "0.119449\n",
      "0.110618\n",
      "0.113349\n",
      "0.117712\n",
      "0.118536\n",
      "0.120989\n",
      "0.117589\n",
      "0.119782\n",
      "0.105993\n",
      "0.108165\n",
      "0.114302\n",
      "0.106692\n",
      "0.108775\n",
      "0.11168\n",
      "0.111673\n",
      "0.116852\n",
      "0.0998304\n",
      "0.106776\n",
      "0.107363\n",
      "0.112222\n",
      "0.102553\n",
      "0.103368\n",
      "0.107866\n",
      "0.105351\n",
      "0.108896\n",
      "0.110082\n",
      "0.106926\n",
      "0.10488\n",
      "0.102209\n",
      "0.106669\n",
      "0.106733\n",
      "0.102531\n",
      "0.102412\n",
      "0.1056\n",
      "0.104745\n",
      "0.1035\n",
      "0.105801\n",
      "0.10342\n",
      "0.102997\n",
      "0.105357\n",
      "0.102141\n",
      "0.105522\n",
      "0.101271\n",
      "0.103773\n",
      "0.105206\n",
      "0.102426\n",
      "0.106546\n",
      "0.0974485\n",
      "0.0995874\n",
      "0.096758\n",
      "0.0999864\n",
      "0.104476\n",
      "0.107956\n",
      "0.0971285\n",
      "0.101678\n",
      "0.0978234\n",
      "0.0983113\n",
      "0.104841\n",
      "0.0945932\n",
      "0.104656\n",
      "0.101419\n",
      "0.102685\n",
      "0.0980261\n",
      "0.0991481\n",
      "0.103081\n",
      "0.107235\n",
      "0.0954452\n",
      "0.0967238\n",
      "0.0964978\n",
      "0.0987922\n",
      "0.0963343\n",
      "0.0929259\n",
      "0.10174\n",
      "0.0971014\n",
      "0.104079\n",
      "0.10566\n",
      "0.0927804\n",
      "0.0949865\n",
      "0.102957\n",
      "0.0960513\n",
      "0.0967898\n",
      "0.0975145\n",
      "0.105926\n",
      "0.0980664\n",
      "0.0893419\n",
      "0.0924371\n",
      "0.100966\n",
      "0.0950767\n",
      "0.096179\n",
      "0.0995884\n",
      "0.0991759\n",
      "0.101132\n",
      "0.0947185\n",
      "0.0981961\n",
      "0.103687\n",
      "0.101721\n",
      "0.106425\n",
      "0.0966564\n",
      "0.0930934\n",
      "0.0858411\n",
      "0.0940436\n",
      "0.0925769\n",
      "0.0903293\n",
      "0.0929978\n",
      "0.0991323\n",
      "0.0886806\n",
      "0.0896867\n",
      "0.0898259\n",
      "0.101986\n",
      "0.0952751\n",
      "0.100897\n",
      "0.0919127\n",
      "0.102144\n",
      "0.0959336\n",
      "0.0908899\n",
      "0.0948738\n",
      "0.089524\n",
      "0.0960586\n",
      "0.0925255\n",
      "0.0901153\n",
      "0.096944\n",
      "0.0923058\n",
      "0.0877901\n",
      "0.0932605\n",
      "0.0968489\n",
      "0.0923718\n",
      "0.0901809\n",
      "0.0939252\n",
      "0.0926933\n",
      "0.0871706\n",
      "0.0982981\n",
      "0.0918435\n",
      "0.0912976\n",
      "0.0990295\n",
      "0.0853138\n",
      "0.0888702\n",
      "0.0907762\n",
      "0.0964382\n",
      "0.0942687\n",
      "0.105767\n",
      "0.0819603\n",
      "0.0904213\n",
      "0.0905734\n",
      "0.0934468\n",
      "0.0896006\n",
      "0.0938535\n",
      "0.0951064\n",
      "0.0919309\n",
      "0.0909268\n",
      "0.0880796\n",
      "0.0931163\n",
      "0.0959035\n",
      "0.0911634\n",
      "0.0960158\n",
      "0.087469\n",
      "0.0909764\n",
      "0.0881788\n",
      "0.0888866\n",
      "0.097347\n",
      "0.0918067\n",
      "0.082976\n",
      "0.0877831\n",
      "0.0884933\n",
      "0.0905777\n",
      "0.0902128\n",
      "0.0908941\n",
      "0.0951664\n",
      "0.0902741\n",
      "0.092857\n",
      "0.0883351\n",
      "0.0857639\n",
      "0.100408\n",
      "0.0882904\n",
      "0.091018\n",
      "0.0857028\n",
      "0.0898471\n",
      "0.102197\n",
      "0.0843999\n",
      "0.0964404\n",
      "0.0915163\n",
      "0.0857863\n",
      "0.0895446\n",
      "0.0947218\n",
      "0.090742\n",
      "0.0845438\n",
      "0.09767\n",
      "0.0923394\n",
      "0.08784\n",
      "0.0917393\n",
      "0.0840427\n",
      "0.0912732\n",
      "0.0868927\n",
      "0.0934531\n",
      "0.0916637\n",
      "0.0932126\n",
      "0.0820748\n",
      "0.0937057\n",
      "0.0899929\n",
      "0.0847226\n",
      "0.0902192\n",
      "0.0920302\n",
      "0.0922863\n",
      "0.0822221\n",
      "0.0874161\n",
      "0.0849861\n",
      "0.0952459\n",
      "0.0938592\n",
      "0.0851392\n",
      "0.0851147\n",
      "0.0945085\n",
      "0.0873815\n",
      "0.0795976\n",
      "0.0909884\n",
      "0.08922\n",
      "0.0917853\n",
      "0.0866769\n",
      "0.088648\n",
      "0.0923447\n",
      "0.0891631\n",
      "0.093746\n",
      "0.0961347\n",
      "0.0935558\n",
      "0.0904891\n",
      "0.0828876\n",
      "0.0853627\n",
      "0.091565\n",
      "0.0934543\n",
      "0.091377\n",
      "0.0842393\n",
      "0.0860267\n",
      "0.0821395\n",
      "0.0888881\n",
      "0.0913191\n",
      "0.094619\n",
      "0.0797035\n",
      "0.0918962\n",
      "0.0875233\n",
      "0.0812584\n",
      "0.0865606\n",
      "0.0944383\n",
      "0.084387\n",
      "0.0937077\n",
      "0.0822116\n",
      "0.0921934\n",
      "0.0858141\n",
      "0.0970027\n",
      "0.0816208\n",
      "0.0928361\n",
      "0.0860873\n",
      "0.0877916\n",
      "0.0797173\n",
      "0.0879754\n",
      "0.090279\n",
      "0.0982763\n",
      "0.0883403\n",
      "0.0864279\n",
      "0.0892344\n",
      "0.0863773\n",
      "0.0882015\n",
      "0.0883466\n",
      "0.0809446\n",
      "0.09073\n",
      "0.0856704\n",
      "0.083171\n",
      "0.0986615\n",
      "0.0841601\n",
      "0.0843336\n",
      "0.0914216\n",
      "0.0853516\n",
      "0.0855585\n",
      "0.0797183\n",
      "0.0905797\n",
      "0.0966682\n",
      "0.0826949\n",
      "0.0844036\n",
      "0.0848742\n",
      "0.0959447\n",
      "0.0963712\n",
      "0.0943021\n",
      "0.0970716\n",
      "0.0877916\n",
      "0.0908753\n",
      "0.0905342\n",
      "0.0849669\n",
      "0.0813109\n",
      "0.0829934\n",
      "0.0891912\n",
      "0.0868531\n",
      "0.0887856\n",
      "0.0895085\n",
      "0.0880526\n",
      "0.091821\n",
      "0.0916674\n",
      "0.0971744\n",
      "0.0925756\n",
      "0.0908413\n",
      "0.085908\n",
      "0.0869363\n",
      "0.0935286\n",
      "0.0862354\n",
      "0.0835338\n",
      "0.0920836\n",
      "0.0891404\n",
      "0.0825582\n",
      "0.0880324\n",
      "0.0847594\n",
      "0.0884537\n",
      "0.0872845\n",
      "0.0856913\n",
      "0.0882223\n",
      "0.084833\n",
      "0.0842046\n",
      "0.0835917\n",
      "0.0892533\n",
      "0.0793848\n",
      "0.087703\n",
      "0.0862339\n",
      "0.0879521\n",
      "0.080305\n",
      "0.0878104\n",
      "0.0851354\n",
      "0.0771099\n",
      "0.0921285\n",
      "0.0900457\n",
      "0.0857524\n",
      "0.0918544\n",
      "0.0913084\n",
      "0.091088\n",
      "0.087207\n",
      "0.0928156\n",
      "0.0867899\n",
      "0.0820896\n",
      "0.0834564\n",
      "0.0965905\n",
      "0.0955009\n",
      "0.0800258\n",
      "0.0897576\n",
      "0.0871817\n",
      "0.0861154\n",
      "0.0931876\n",
      "0.0858908\n",
      "0.0939129\n",
      "0.0975786\n",
      "0.0930592\n",
      "0.0934539\n",
      "0.0935349\n",
      "0.0928387\n",
      "0.0885134\n",
      "0.0883559\n",
      "0.0868419\n",
      "0.0840039\n",
      "0.0851721\n",
      "0.086234\n",
      "0.087647\n",
      "0.0908103\n",
      "0.0915529\n",
      "0.0871268\n",
      "0.0767241\n",
      "0.0928369\n",
      "0.0882516\n",
      "0.0888744\n",
      "0.0896986\n",
      "0.088619\n",
      "0.0875763\n",
      "0.0896623\n",
      "0.0906044\n",
      "0.0951326\n",
      "0.0857155\n",
      "0.0903093\n",
      "0.0859621\n",
      "0.0934937\n",
      "0.0843213\n",
      "0.0856658\n",
      "0.0958844\n",
      "0.0829636\n",
      "0.0850183\n",
      "0.0955315\n",
      "0.091685\n",
      "0.082741\n",
      "0.0924192\n",
      "0.0810853\n",
      "0.0792136\n",
      "0.0914246\n",
      "0.0923388\n",
      "0.085657\n",
      "0.0792621\n",
      "0.0878371\n",
      "0.090953\n",
      "0.0860334\n",
      "0.080807\n",
      "0.0853647\n",
      "0.0790127\n",
      "0.0875314\n",
      "0.0881671\n",
      "0.0828527\n",
      "0.0916056\n",
      "0.0874025\n",
      "0.0915743\n",
      "0.0867673\n",
      "0.0833401\n",
      "0.0873531\n",
      "0.0925011\n",
      "0.0930609\n",
      "0.0884356\n",
      "0.0812407\n",
      "0.0890191\n",
      "0.0828912\n",
      "0.086073\n",
      "0.0846718\n",
      "0.0814647\n",
      "0.0846801\n",
      "0.089461\n",
      "0.0865894\n",
      "0.0815743\n",
      "0.0882449\n",
      "0.0876366\n",
      "0.0877445\n",
      "0.0914828\n",
      "0.077035\n",
      "0.0941718\n",
      "0.0759871\n",
      "0.0932562\n",
      "0.0864793\n",
      "0.087991\n",
      "0.0791938\n",
      "0.0926769\n",
      "0.0798889\n",
      "0.0990679\n",
      "0.0863773\n",
      "0.0825707\n",
      "0.0878427\n",
      "0.0890097\n",
      "0.0989841\n",
      "0.0782906\n",
      "0.0917025\n",
      "0.0859642\n",
      "0.0859927\n",
      "0.0824209\n",
      "0.087606\n",
      "0.0955725\n",
      "0.0944989\n",
      "0.0832941\n",
      "0.0844957\n",
      "0.0844037\n",
      "0.0931534\n",
      "0.0945491\n",
      "0.0794439\n",
      "0.0850674\n",
      "0.0858152\n",
      "0.0865854\n",
      "0.0839053\n",
      "0.0970514\n",
      "0.0852038\n",
      "0.0808556\n",
      "0.0816453\n",
      "0.101636\n",
      "0.0885448\n",
      "0.0899989\n",
      "0.0852695\n",
      "0.0798073\n",
      "0.0838982\n",
      "0.0899111\n",
      "0.0834978\n",
      "0.0866009\n",
      "0.0869994\n",
      "0.0871989\n",
      "0.0929054\n",
      "0.0847024\n",
      "0.0844303\n",
      "0.0898926\n",
      "0.0839084\n",
      "0.087309\n",
      "0.0892195\n",
      "0.0776061\n",
      "0.0848188\n",
      "0.0852929\n",
      "0.0794933\n",
      "0.0817094\n",
      "0.0824043\n",
      "0.0899372\n",
      "0.0894048\n",
      "0.0924869\n",
      "0.0906909\n",
      "0.0893215\n",
      "0.08269\n",
      "0.0866566\n",
      "0.091116\n",
      "0.0870849\n",
      "0.0833573\n",
      "0.091691\n",
      "0.0812853\n",
      "0.0893696\n",
      "0.091372\n",
      "0.0783798\n",
      "0.0889956\n",
      "0.081226\n",
      "0.0896546\n",
      "0.0946857\n",
      "0.0869667\n",
      "0.0848902\n",
      "0.085888\n",
      "0.0923759\n",
      "0.0920219\n",
      "0.0897106\n",
      "0.0878827\n",
      "0.0869768\n",
      "0.0859067\n",
      "0.0858121\n",
      "0.0888392\n",
      "0.0827741\n",
      "0.0865458\n",
      "0.0781329\n",
      "0.0946218\n",
      "0.0867481\n",
      "0.0866643\n",
      "0.0826855\n",
      "0.0864648\n",
      "0.0952016\n",
      "0.0828809\n",
      "0.0853031\n",
      "0.0909295\n",
      "0.0840806\n",
      "0.0746575\n",
      "0.0794807\n",
      "0.0907021\n",
      "0.0856365\n",
      "0.0873984\n",
      "0.0833921\n",
      "0.0832634\n",
      "0.0904775\n",
      "0.0924573\n",
      "0.0826729\n",
      "0.0808755\n",
      "0.0920079\n",
      "0.0836027\n",
      "0.0846377\n",
      "0.0805649\n",
      "0.0891371\n",
      "0.071506\n",
      "0.0951926\n",
      "0.0869965\n",
      "0.0850807\n",
      "0.0859065\n",
      "0.0873082\n",
      "0.0863037\n",
      "0.0911778\n",
      "0.079877\n",
      "0.0919211\n",
      "0.0811495\n",
      "0.0844489\n",
      "0.0821548\n",
      "0.0864776\n",
      "0.0881145\n",
      "0.0890021\n",
      "0.094062\n",
      "0.0895074\n",
      "0.0957618\n",
      "0.0907437\n",
      "0.0890586\n",
      "0.0925003\n",
      "0.0798694\n",
      "0.0861062\n",
      "0.0843836\n",
      "0.0835051\n",
      "0.0955607\n",
      "0.0816936\n",
      "0.089288\n",
      "0.0906227\n",
      "0.0822544\n",
      "0.0792463\n",
      "0.096791\n",
      "0.0842388\n",
      "0.0866225\n",
      "0.0920593\n",
      "0.0854637\n",
      "0.0833958\n",
      "0.0864811\n",
      "0.0820859\n",
      "0.0870312\n",
      "0.0846424\n",
      "0.0797833\n",
      "0.0838152\n",
      "0.0921777\n",
      "0.0905192\n",
      "0.0887347\n",
      "0.0853492\n",
      "0.0824381\n",
      "0.0908386\n",
      "0.0912273\n",
      "0.0923698\n",
      "0.0800822\n",
      "0.0816937\n",
      "0.0918797\n",
      "0.0777131\n",
      "0.0861765\n",
      "0.0847386\n",
      "0.0875914\n",
      "0.0868615\n",
      "0.0865633\n",
      "0.0895431\n",
      "0.0751579\n",
      "0.0872199\n",
      "0.0859262\n",
      "0.0818771\n",
      "0.0913785\n",
      "0.0951834\n",
      "0.0839044\n",
      "0.0948736\n",
      "0.0773784\n",
      "0.0858229\n",
      "0.0814758\n",
      "0.0858173\n",
      "0.0859683\n",
      "0.0870479\n",
      "0.090378\n",
      "0.0846523\n",
      "0.0852943\n",
      "0.0901492\n",
      "0.0849017\n",
      "0.0846049\n",
      "0.0909603\n",
      "0.0983896\n",
      "0.0842777\n",
      "0.0897246\n",
      "0.0843676\n",
      "0.0879477\n",
      "0.0866786\n",
      "0.0917736\n",
      "0.0873073\n",
      "0.0884823\n",
      "0.0824049\n",
      "0.080469\n",
      "0.0871358\n",
      "0.0831294\n",
      "0.0895243\n",
      "0.0859758\n",
      "0.0806509\n",
      "0.0906317\n",
      "0.0872375\n",
      "0.0862985\n",
      "0.091421\n",
      "0.0779727\n",
      "0.09306\n",
      "0.087323\n",
      "0.0909801\n",
      "0.0825203\n",
      "0.0836928\n",
      "0.0977102\n",
      "0.0920791\n",
      "0.0815624\n",
      "0.0868906\n",
      "0.0847248\n",
      "0.0845989\n",
      "0.0804335\n",
      "0.0813703\n",
      "0.0897687\n",
      "0.0917434\n",
      "0.0911282\n",
      "0.0854492\n",
      "0.0864123\n",
      "0.082054\n",
      "0.0864438\n",
      "0.0864831\n",
      "0.0886537\n",
      "0.0926243\n",
      "0.0877959\n",
      "0.0899376\n",
      "0.0821237\n",
      "0.0806013\n",
      "0.0836742\n",
      "0.0847454\n",
      "0.0827109\n",
      "0.0870977\n",
      "0.0899821\n",
      "0.0876644\n",
      "0.080976\n",
      "0.0873411\n",
      "0.0899735\n",
      "0.089297\n",
      "0.0809845\n",
      "0.085519\n",
      "0.0858032\n",
      "0.088426\n",
      "0.0883402\n",
      "0.091123\n",
      "0.0897793\n",
      "0.0883829\n",
      "0.0811341\n",
      "0.0864263\n",
      "0.0828484\n",
      "0.0881908\n",
      "0.0872478\n",
      "0.0879205\n",
      "0.087944\n",
      "0.0959374\n",
      "0.0940312\n",
      "0.0833634\n",
      "0.0852884\n",
      "0.0857978\n",
      "0.0843356\n",
      "0.0906901\n",
      "0.0887048\n",
      "0.0892264\n",
      "0.0866669\n",
      "0.0887381\n",
      "0.0818222\n",
      "0.0865746\n",
      "0.0835226\n",
      "0.0895409\n",
      "0.08923\n",
      "0.0820648\n",
      "0.0914059\n",
      "0.0801858\n",
      "0.0894215\n",
      "0.0957174\n",
      "0.0891966\n",
      "0.0895286\n",
      "0.0872666\n",
      "0.0881676\n",
      "0.0863613\n",
      "0.0869238\n",
      "0.0871987\n",
      "0.0844224\n",
      "0.0904301\n",
      "0.0811376\n",
      "0.0828051\n",
      "0.0884602\n",
      "0.0827394\n",
      "0.087702\n",
      "0.0806617\n",
      "0.0814366\n",
      "0.08972\n",
      "0.0952381\n",
      "0.0907362\n",
      "0.0805142\n",
      "0.0932608\n",
      "0.0798232\n",
      "0.0920455\n",
      "0.0942466\n",
      "0.0872479\n",
      "0.09232\n",
      "0.0858005\n",
      "0.0833903\n",
      "0.0779363\n",
      "0.0866119\n",
      "0.0763202\n",
      "0.0915269\n",
      "0.0857945\n",
      "0.0921267\n",
      "0.0822985\n",
      "0.0929821\n",
      "0.0899237\n",
      "0.0897259\n",
      "0.0894115\n",
      "0.0801903\n",
      "0.086408\n",
      "0.0832072\n",
      "0.0819876\n",
      "0.0856866\n",
      "0.0827298\n",
      "0.0778964\n",
      "0.0896128\n",
      "0.0881924\n",
      "0.0844784\n",
      "0.0796682\n",
      "0.0824838\n",
      "0.0812715\n",
      "0.0849925\n",
      "0.0898\n",
      "0.0820064\n",
      "0.087696\n",
      "0.0838152\n",
      "0.0806747\n",
      "0.0904634\n",
      "0.0875224\n",
      "0.0886535\n",
      "0.0812162\n",
      "0.0903642\n",
      "0.0747346\n",
      "0.0855601\n",
      "0.0866206\n",
      "0.087901\n",
      "0.0888619\n",
      "0.0915384\n",
      "0.0769223\n",
      "0.0823767\n",
      "0.0879416\n",
      "0.0977507\n",
      "0.0892385\n",
      "0.0884743\n",
      "0.0909842\n",
      "0.0852751\n",
      "0.0823902\n",
      "0.0907658\n",
      "0.0837197\n",
      "0.091679\n",
      "0.0834907\n",
      "0.087752\n",
      "0.0922681\n",
      "0.0873848\n",
      "0.0902944\n",
      "0.0954078\n",
      "0.0827008\n",
      "0.0849425\n",
      "0.0740994\n",
      "0.0835556\n",
      "0.0832873\n",
      "0.0865754\n",
      "0.086062\n",
      "0.0860552\n",
      "0.0875375\n",
      "0.0876345\n",
      "0.0942088\n",
      "0.0878669\n",
      "0.0831934\n",
      "0.0905739\n",
      "0.0811948\n",
      "0.0889412\n",
      "0.0861031\n",
      "0.0868471\n",
      "0.0834657\n",
      "0.0830107\n",
      "0.0903118\n",
      "0.0839705\n",
      "0.0973336\n",
      "0.0846258\n",
      "0.092976\n",
      "0.0866323\n",
      "0.0775161\n",
      "0.0881779\n",
      "0.0922486\n",
      "0.0896603\n",
      "0.0848492\n",
      "0.092187\n",
      "0.0915536\n",
      "0.0813367\n",
      "0.0895134\n",
      "0.0940591\n",
      "0.088973\n",
      "0.0800943\n",
      "0.0857449\n",
      "0.0818366\n",
      "0.0841644\n",
      "0.0907084\n",
      "0.0802961\n",
      "0.0894413\n",
      "0.0915615\n",
      "0.0803868\n",
      "0.0854642\n",
      "0.0859474\n",
      "0.0832373\n",
      "0.0933106\n",
      "0.0852026\n",
      "0.0849414\n",
      "0.0851584\n",
      "0.0822269\n",
      "0.087067\n",
      "0.0918722\n",
      "0.0982208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0855808\n",
      "0.0869421\n",
      "0.0867147\n",
      "0.089274\n",
      "0.0864177\n",
      "0.0815635\n",
      "0.0808941\n",
      "0.0892221\n",
      "0.0886998\n",
      "0.088511\n",
      "0.0747403\n",
      "0.081802\n",
      "0.0846736\n",
      "0.0816763\n",
      "0.0855448\n",
      "0.0751417\n",
      "0.0855743\n",
      "0.0887482\n",
      "0.0800262\n",
      "0.090799\n",
      "0.088768\n",
      "0.0848094\n",
      "0.0820833\n",
      "0.0820675\n",
      "0.0851991\n",
      "0.0834472\n",
      "0.0850621\n",
      "0.0821621\n",
      "0.0821177\n",
      "0.0854876\n",
      "0.0827134\n",
      "0.0849108\n",
      "0.0752177\n",
      "0.0945526\n",
      "0.0849908\n",
      "0.0808758\n",
      "0.0795425\n",
      "0.0903826\n",
      "0.0845835\n",
      "0.0937522\n",
      "0.0843983\n",
      "0.0872494\n",
      "0.0870872\n",
      "0.0887388\n",
      "0.0831373\n",
      "0.0842074\n",
      "0.0898327\n",
      "0.0838722\n",
      "0.091605\n",
      "0.0901324\n",
      "0.094348\n",
      "0.0848101\n",
      "0.0779868\n",
      "0.0843227\n",
      "0.0809447\n",
      "0.0893365\n",
      "0.0917707\n",
      "0.0837512\n",
      "0.0910984\n",
      "0.0820611\n",
      "0.0842039\n",
      "0.0870417\n",
      "0.0820314\n",
      "0.0859067\n",
      "0.08857\n",
      "0.0851985\n",
      "0.0869954\n",
      "0.0899315\n",
      "0.0852669\n",
      "0.0867072\n",
      "0.0828459\n",
      "0.0798526\n",
      "0.0836076\n",
      "0.0957862\n",
      "0.0905799\n",
      "0.088625\n",
      "0.0858608\n",
      "0.079524\n",
      "0.0878601\n",
      "0.0765023\n",
      "0.0808259\n",
      "0.0827707\n",
      "0.0783869\n",
      "0.0854916\n",
      "0.0878747\n",
      "0.0843708\n",
      "0.0841914\n",
      "0.0793877\n",
      "0.0839092\n",
      "0.0869571\n",
      "0.0833443\n",
      "0.0855082\n",
      "0.0924434\n",
      "0.0857357\n",
      "0.0867898\n",
      "0.0851318\n",
      "0.0831853\n",
      "0.0797339\n",
      "0.0917525\n",
      "0.0852299\n",
      "0.0943061\n",
      "0.0816527\n",
      "0.0824499\n",
      "0.0857527\n",
      "0.0867809\n",
      "0.078892\n",
      "0.0886957\n",
      "0.0832616\n",
      "0.0959335\n",
      "0.087006\n",
      "0.0834317\n",
      "0.0829403\n",
      "0.0840846\n",
      "0.0858868\n",
      "0.0881839\n",
      "0.0776145\n",
      "0.0791777\n",
      "0.084443\n",
      "0.091925\n",
      "0.085051\n",
      "0.0835174\n",
      "0.084072\n",
      "0.0832985\n",
      "0.084397\n",
      "0.088498\n",
      "0.0820303\n",
      "0.0900839\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*- \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1.训练的数据\n",
    "\n",
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise\n",
    "\n",
    "# 2.定义节点准备接收数据\n",
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# 3.定义神经层：隐藏层和预测层\n",
    "# 添加层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob=0.5)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元   \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "# 4.定义 loss 表达式  \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                     reduction_indices=[1]))\n",
    "\n",
    "# 5.选择 optimizer 使 loss 达到最小                   \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1       \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# important step 对所有变量进行初始化\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算\n",
    "sess.run(init)\n",
    "\n",
    "# 迭代 1000 次学习，sess.run optimizer\n",
    "for i in range(1000):\n",
    "    # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
